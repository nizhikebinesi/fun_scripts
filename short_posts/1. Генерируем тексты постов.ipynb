{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой тетрадке мы будем создавать тексты постов.\n",
    "\n",
    "Начнем с простого способа, а закончим генерацией текстов с помощью нейросетей. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Марковские цепи(из коробки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texts(name='data.txt'):\n",
    "    with open(name) as f:\n",
    "        return [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = load_texts()\n",
    "chain = markovify.Text(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем 7 предложений при помощи полученной марковской цепи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1:\n",
      "А ведь самые красивые девушки — только у нас!\n",
      "\n",
      "sentence 2:\n",
      "А ведь самые красивые девушки учатся на матфаке!\n",
      "\n",
      "sentence 3:\n",
      "А ведь самые красивые девушки учатся на матфаке.\n",
      "\n",
      "sentence 4:\n",
      "Еще помните, где учатся самые красивые девушки учатся на матфаке!И, между прочим, хорошо учатся!\n",
      "\n",
      "sentence 5:\n",
      "Еще помните, где учатся самые красивые девушки учатся у нас.\n",
      "\n",
      "sentence 6:\n",
      "А ведь самые красивые девушки?\n",
      "\n",
      "sentence 7:\n",
      "А ведь самые красивые девушки учатся на матфаке!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 7\n",
    "i = 0\n",
    "while True:\n",
    "    if i >= n:\n",
    "        break\n",
    "    sent = chain.make_sentence()\n",
    "    if sent is None:\n",
    "        continue\n",
    "    print('sentence %d:' % (i + 1))\n",
    "    print(sent, end=\"\\n\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы немного увеличить датасет будем использовать выкачанные эпитеты(в прошлой тетрадке проводилась слабая предобработка)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjs = load_texts('some_adj.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(s):\n",
    "    rows = [word_tokenize(x) for x in s]\n",
    "    strs = []\n",
    "    for r in rows:\n",
    "        for i in range(len(r)):\n",
    "            if morph.tag(r[i])[0].POS == 'ADJF':\n",
    "                for j in range(50):\n",
    "                    t = r.copy()\n",
    "            \n",
    "                    tag = morph.tag(t[i])[0]\n",
    "                    gramms = tag.grammemes\n",
    "                    rnd_word = morph.parse(np.random.choice(adjs, replace=True))[0]\n",
    "                    new_word = rnd_word.inflect(gramms)\n",
    "                    if new_word is None:\n",
    "                        continue\n",
    "                    t[i] = new_word.word\n",
    "                    string = u''\n",
    "                    for j in range(len(r)):\n",
    "                        if str(morph.tag(t[j])[0]) == 'PNCT':\n",
    "                            string += t[j]\n",
    "                        else:\n",
    "                            string += u' ' + t[j]\n",
    "                    strs.append(string.strip())\n",
    "    strs.extend(s)\n",
    "    return strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.97 s, sys: 0 ns, total: 4.97 s\n",
      "Wall time: 5.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "augmented = augment(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_texts(texts, name='augmented.txt'):\n",
    "    with open(name, 'w') as f:\n",
    "        f.write('\\n'.join(texts))\n",
    "        \n",
    "save_texts(augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взглянем на результат!.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['А ведь самые удивительные девушки учатся у нас!',\n",
       " 'А ведь самые креативные девушки учатся у нас!']"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = markovify.Text(augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1:\n",
      "Самые восточные и удивительные девушки учатся на матфаке и ходят в фирменных футболках ИМИТ!\n",
      "\n",
      "sentence 2:\n",
      "А ведь самые сердечные девушки учатся на матфаке.\n",
      "\n",
      "sentence 3:\n",
      "А ведь самые лихие девушки учатся на матфаке?\n",
      "\n",
      "sentence 4:\n",
      "Самые добрые и удивительные девушки учатся у нас.\n",
      "\n",
      "sentence 5:\n",
      "Самые честолюбивые девушки учатся у нас!\n",
      "\n",
      "sentence 6:\n",
      "Самые обманчивые девушки учатся у нас!\n",
      "\n",
      "sentence 7:\n",
      "Самые пластичные девушки учатся у нас.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 7\n",
    "i = 0\n",
    "while True:\n",
    "    if i >= n:\n",
    "        break\n",
    "    sent = chain1.make_sentence()\n",
    "    if sent is None:\n",
    "        continue\n",
    "    print('sentence %d:' % (i + 1))\n",
    "    print(sent, end=\"\\n\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хех 8)<br>\n",
    "Забавно, хорошечно!\n",
    "\n",
    "Теперь попробуем более замудренный вариант генерации подобных постов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Char-level RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный фрагмент тетрадки основан на <a href=\"https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\">туторе из документации PyTorch</a>, но наш пример проще.\n",
    "\n",
    "Натравим Vanilla RNN на генерацию постов!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "all_chars = ''.join(set(''.join(augmented)))\n",
    "n_chars = len(all_chars) + 1\n",
    "print(n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_example():\n",
    "    text = augmented[np.random.randint(0, len(augmented))]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "def inputTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_chars)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "        tensor[li][0][all_chars.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# LongTensor of second letter to end (EOS) for target\n",
    "def targetTensor(line):\n",
    "    letter_indexes = [all_chars.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_chars - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make category, input, and target tensors from a random category, line pair\n",
    "def randomTrainingExample():\n",
    "    line = random_example()\n",
    "    input_line_tensor = inputTensor(line)\n",
    "    target_line_tensor = targetTensor(line)\n",
    "    return input_line_tensor, target_line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input_combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        output_combined = torch.cat((hidden, output), 1)\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "def train(input_line_tensor, target_line_tensor):\n",
    "    target_line_tensor.unsqueeze_(-1)\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(input_line_tensor.size(0)):\n",
    "        output, hidden = rnn(input_line_tensor[i], hidden)\n",
    "        l = criterion(output, target_line_tensor[i])\n",
    "        loss += l\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(rnn.parameters(), 150)\n",
    "    \n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item() / input_line_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2cf8ed2b1e4230964e3e498a000e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 33s (500 0%) 3.4234\n",
      "1m 12s (1000 1%) 2.5197\n",
      "1m 56s (1500 1%) 1.3148\n",
      "2m 37s (2000 2%) 1.1514\n",
      "3m 19s (2500 2%) 1.7860\n",
      "4m 7s (3000 3%) 1.4363\n",
      "4m 49s (3500 3%) 1.7283\n",
      "5m 30s (4000 4%) 1.1062\n",
      "6m 17s (4500 4%) 0.8738\n",
      "7m 4s (5000 5%) 1.0150\n",
      "7m 51s (5500 5%) 0.8243\n",
      "8m 40s (6000 6%) 0.5487\n",
      "9m 28s (6500 6%) 0.6510\n",
      "10m 11s (7000 7%) 1.7874\n",
      "10m 57s (7500 7%) 1.0171\n",
      "11m 46s (8000 8%) 1.0037\n",
      "12m 32s (8500 8%) 1.3180\n",
      "13m 16s (9000 9%) 0.4659\n",
      "13m 57s (9500 9%) 1.3168\n",
      "14m 50s (10000 10%) 1.2021\n",
      "15m 39s (10500 10%) 0.7185\n",
      "16m 26s (11000 11%) 0.9242\n",
      "17m 14s (11500 11%) 0.7445\n",
      "18m 2s (12000 12%) 1.5318\n",
      "18m 54s (12500 12%) 1.2056\n",
      "19m 44s (13000 13%) 0.8723\n",
      "20m 29s (13500 13%) 1.0718\n",
      "21m 21s (14000 14%) 1.1220\n",
      "22m 15s (14500 14%) 1.1002\n",
      "22m 57s (15000 15%) 0.7105\n",
      "23m 45s (15500 15%) 0.4661\n",
      "24m 33s (16000 16%) 0.9879\n",
      "25m 25s (16500 16%) 0.8762\n",
      "26m 10s (17000 17%) 0.9272\n",
      "26m 57s (17500 17%) 0.6406\n",
      "27m 46s (18000 18%) 0.8268\n",
      "28m 35s (18500 18%) 1.2909\n",
      "29m 19s (19000 19%) 0.7623\n",
      "30m 7s (19500 19%) 0.8667\n",
      "30m 53s (20000 20%) 0.6358\n",
      "31m 43s (20500 20%) 0.8353\n",
      "32m 24s (21000 21%) 0.6053\n",
      "33m 15s (21500 21%) 0.7719\n",
      "34m 4s (22000 22%) 0.7247\n",
      "34m 51s (22500 22%) 0.8548\n",
      "35m 41s (23000 23%) 0.6404\n",
      "36m 30s (23500 23%) 1.2848\n",
      "37m 18s (24000 24%) 0.7906\n",
      "38m 4s (24500 24%) 0.9635\n",
      "38m 48s (25000 25%) 0.9561\n",
      "39m 37s (25500 25%) 0.5850\n",
      "40m 24s (26000 26%) 0.5747\n",
      "41m 10s (26500 26%) 0.6433\n",
      "42m 2s (27000 27%) 0.7091\n",
      "42m 47s (27500 27%) 0.9365\n",
      "43m 31s (28000 28%) 0.8655\n",
      "44m 16s (28500 28%) 0.6733\n",
      "45m 7s (29000 28%) 0.4779\n",
      "46m 0s (29500 29%) 0.6857\n",
      "46m 46s (30000 30%) 0.3671\n",
      "47m 33s (30500 30%) 0.5952\n",
      "48m 16s (31000 31%) 0.4944\n",
      "49m 3s (31500 31%) 0.5103\n",
      "49m 53s (32000 32%) 1.0202\n",
      "50m 36s (32500 32%) 0.5017\n",
      "51m 26s (33000 33%) 0.9878\n",
      "52m 12s (33500 33%) 0.8959\n",
      "52m 56s (34000 34%) 1.0772\n",
      "53m 44s (34500 34%) 0.9346\n",
      "54m 33s (35000 35%) 0.7115\n",
      "55m 21s (35500 35%) 0.8603\n",
      "56m 10s (36000 36%) 1.1449\n",
      "56m 56s (36500 36%) 0.4955\n",
      "57m 48s (37000 37%) 1.0604\n",
      "58m 35s (37500 37%) 0.8340\n",
      "59m 18s (38000 38%) 0.6271\n",
      "60m 12s (38500 38%) 0.6965\n",
      "60m 59s (39000 39%) 0.7567\n",
      "61m 44s (39500 39%) 0.4786\n",
      "62m 30s (40000 40%) 0.8620\n",
      "63m 22s (40500 40%) 0.8076\n",
      "64m 11s (41000 41%) 0.6032\n",
      "65m 6s (41500 41%) 0.9326\n",
      "65m 55s (42000 42%) 0.6492\n",
      "66m 45s (42500 42%) 1.1809\n",
      "67m 39s (43000 43%) 0.8264\n",
      "68m 26s (43500 43%) 1.0285\n",
      "69m 19s (44000 44%) 0.8313\n",
      "70m 9s (44500 44%) 1.1031\n",
      "70m 57s (45000 45%) 0.6704\n",
      "71m 42s (45500 45%) 0.5289\n",
      "72m 29s (46000 46%) 0.9539\n",
      "73m 18s (46500 46%) 0.8181\n",
      "74m 4s (47000 47%) 0.8240\n",
      "74m 58s (47500 47%) 0.4693\n",
      "75m 48s (48000 48%) 0.7822\n",
      "76m 36s (48500 48%) 0.6116\n",
      "77m 26s (49000 49%) 0.5058\n",
      "78m 15s (49500 49%) 0.9405\n",
      "79m 3s (50000 50%) 0.5959\n",
      "79m 50s (50500 50%) 0.4733\n",
      "80m 39s (51000 51%) 0.6445\n",
      "81m 31s (51500 51%) 0.4313\n",
      "82m 18s (52000 52%) 0.4939\n",
      "83m 10s (52500 52%) 0.9988\n",
      "83m 56s (53000 53%) 1.2374\n",
      "84m 44s (53500 53%) 0.3506\n",
      "85m 31s (54000 54%) 0.4174\n",
      "86m 18s (54500 54%) 0.6658\n",
      "87m 5s (55000 55%) 0.4629\n",
      "87m 48s (55500 55%) 0.7962\n",
      "88m 31s (56000 56%) 0.6845\n",
      "89m 13s (56500 56%) 0.4606\n",
      "89m 56s (57000 56%) 7.6057\n",
      "90m 39s (57500 57%) 1.0046\n",
      "91m 25s (58000 57%) 0.6454\n",
      "92m 10s (58500 58%) 0.7129\n",
      "92m 56s (59000 59%) 0.4617\n",
      "93m 43s (59500 59%) 0.8576\n",
      "94m 31s (60000 60%) 0.8337\n",
      "95m 19s (60500 60%) 0.6438\n",
      "96m 9s (61000 61%) 0.3238\n",
      "96m 57s (61500 61%) 0.6857\n",
      "97m 42s (62000 62%) 0.4959\n",
      "98m 32s (62500 62%) 0.5063\n",
      "99m 22s (63000 63%) 0.7578\n",
      "100m 11s (63500 63%) 0.6958\n",
      "100m 59s (64000 64%) 0.4105\n",
      "101m 45s (64500 64%) 0.7331\n",
      "102m 34s (65000 65%) 0.4797\n",
      "103m 30s (65500 65%) 0.6161\n",
      "104m 18s (66000 66%) 0.7209\n",
      "105m 6s (66500 66%) 0.6368\n",
      "105m 57s (67000 67%) 0.9718\n",
      "107m 10s (67500 67%) 1.0710\n",
      "108m 24s (68000 68%) 0.4498\n",
      "109m 31s (68500 68%) 0.3193\n",
      "110m 44s (69000 69%) 0.6228\n",
      "111m 55s (69500 69%) 0.4213\n",
      "113m 2s (70000 70%) 0.8436\n",
      "114m 22s (70500 70%) 0.8751\n",
      "115m 34s (71000 71%) 0.6138\n",
      "116m 31s (71500 71%) 0.5106\n",
      "117m 35s (72000 72%) 0.8127\n",
      "118m 39s (72500 72%) 0.4478\n",
      "119m 39s (73000 73%) 0.9022\n",
      "120m 46s (73500 73%) 0.3469\n",
      "121m 52s (74000 74%) 0.4223\n",
      "123m 1s (74500 74%) 0.8078\n",
      "124m 12s (75000 75%) 0.3551\n",
      "125m 17s (75500 75%) 0.5343\n",
      "126m 27s (76000 76%) 0.5364\n",
      "127m 22s (76500 76%) 0.4889\n",
      "128m 46s (77000 77%) 0.4276\n",
      "129m 52s (77500 77%) 0.5273\n",
      "130m 52s (78000 78%) 0.5515\n",
      "132m 4s (78500 78%) 0.7094\n",
      "133m 5s (79000 79%) 0.6821\n",
      "134m 12s (79500 79%) 0.3845\n",
      "135m 13s (80000 80%) 0.7310\n",
      "136m 25s (80500 80%) 0.5179\n",
      "137m 43s (81000 81%) 0.5256\n",
      "138m 53s (81500 81%) 0.4525\n",
      "140m 5s (82000 82%) 0.3141\n",
      "141m 15s (82500 82%) 0.5171\n",
      "142m 25s (83000 83%) 0.5392\n",
      "143m 29s (83500 83%) 0.5224\n",
      "144m 45s (84000 84%) 0.8897\n",
      "146m 1s (84500 84%) 0.6578\n",
      "147m 7s (85000 85%) 1.1456\n",
      "148m 26s (85500 85%) 0.9803\n",
      "149m 36s (86000 86%) 0.7073\n",
      "150m 40s (86500 86%) 0.4054\n",
      "152m 10s (87000 87%) 0.5037\n",
      "153m 20s (87500 87%) 0.7744\n",
      "154m 37s (88000 88%) 0.7416\n",
      "155m 48s (88500 88%) 0.7325\n",
      "157m 0s (89000 89%) 1.1533\n",
      "158m 4s (89500 89%) 0.5239\n",
      "159m 2s (90000 90%) 0.4284\n",
      "160m 15s (90500 90%) 0.7117\n",
      "161m 23s (91000 91%) 0.5759\n",
      "162m 36s (91500 91%) 0.2075\n",
      "163m 51s (92000 92%) 0.7908\n",
      "165m 0s (92500 92%) 0.6224\n",
      "166m 12s (93000 93%) 0.7420\n",
      "167m 25s (93500 93%) 0.3760\n",
      "168m 24s (94000 94%) 0.5018\n",
      "169m 52s (94500 94%) 0.3304\n",
      "171m 11s (95000 95%) 0.7822\n",
      "172m 19s (95500 95%) 0.7576\n",
      "173m 19s (96000 96%) 0.9335\n",
      "174m 34s (96500 96%) 0.7350\n",
      "175m 34s (97000 97%) 0.3526\n",
      "176m 38s (97500 97%) 0.7680\n",
      "177m 38s (98000 98%) 0.6911\n",
      "178m 43s (98500 98%) 0.6705\n",
      "179m 52s (99000 99%) 0.7190\n",
      "181m 1s (99500 99%) 0.6393\n",
      "182m 1s (100000 100%) 0.4033\n",
      "CPU times: user 11h 52min 37s, sys: 8min 12s, total: 12h 49s\n",
      "Wall time: 3h 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "rnn = RNN(n_chars, 128, n_chars)\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 500\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in tqdm_notebook(range(1, n_iters + 1)):\n",
    "    output, loss = train(*randomTrainingExample())\n",
    "    total_loss += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 70\n",
    "\n",
    "\n",
    "def sample(start_letter='С'):\n",
    "    with torch.no_grad():\n",
    "        input = inputTensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            output, hidden = rnn(input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_chars - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = all_chars[topi]\n",
    "                output_name += letter\n",
    "            input = inputTensor(letter)\n",
    "\n",
    "        return output_name\n",
    "\n",
    "    \n",
    "def samples(start_letters='Сскд'):\n",
    "    for start_letter in start_letters:\n",
    "        print(sample(start_letter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самые красивые и нерастие делушке учстся на мо:фаkе инальные девушки уч\n",
      "самнные девушки учатся на мастакиатые девдевушки учатся на матфаке.\n",
      "ковые девушки учатся нанмые ккасивые пельшки учистяена матфаке!\n",
      "давыль сумые изаротные деудивительные девушки учаpся на матфакG!\n"
     ]
    }
   ],
   "source": [
    "samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воу! Получилось прикольно, но жутко.\n",
    "\n",
    "Поэтому давайте вместо Vanilla RNN будем использовать LSTM?(но дальше мы будем генерировать пост из слов, а не посимвольно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on <a href=\"https://medium.freecodecamp.org/applied-introduction-to-lstms-for-text-generation-380158b29fb3\">link #1</a>, <a href=\"https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275\">link #2</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Densу\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = [word_tokenize(x.lower()) for x in augmented]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "def dataset_preparation(data):\n",
    "    corpus = data\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    input_sequences = []\n",
    "    \n",
    "    for line in new_texts:\n",
    "        \n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        \n",
    "        for i in range(1, len(token_list)):\n",
    "            \n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "            \n",
    "            if True:#np.random.normal() > -0.1:\n",
    "                \n",
    "                for j in range(i):\n",
    "                    if abs(j - i) > 2 and abs(j - i) < len(token_list) // 2:\n",
    "                        \n",
    "                        n_gram_sequence = token_list[j:i+1]\n",
    "                        input_sequences.append(n_gram_sequence)\n",
    "    \n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(\n",
    "        pad_sequences(\n",
    "            input_sequences,   \n",
    "            maxlen=max_sequence_len, \n",
    "            padding='pre')\n",
    "    )\n",
    "    predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "    label = np_utils.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    optimizer = RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, max_sequence_len):\n",
    "    for j in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen= \n",
    "                             max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "  \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.46 s, sys: 83.2 ms, total: 1.54 s\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, Y, max_len, total_words = dataset_preparation(new_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(max_len, total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_35 (Embedding)     (None, 53, 10)            3570      \n",
      "_________________________________________________________________\n",
      "lstm_62 (LSTM)               (None, 53, 150)           96600     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 53, 150)           0         \n",
      "_________________________________________________________________\n",
      "lstm_63 (LSTM)               (None, 100)               100400    \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 357)               36057     \n",
      "=================================================================\n",
      "Total params: 236,627\n",
      "Trainable params: 236,627\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % (epoch + 1))\n",
    "    print(generate_text('самые', 54, 54))\n",
    "    print('--------------------------------------')\n",
    "    print(generate_text('красивые', 54, 54))\n",
    "    print('--------------------------------------')\n",
    "    print(generate_text('великолепные', 54, 54))\n",
    "    print('--------------------------------------')\n",
    "    print(generate_text('у нас', 54, 54))\n",
    "    print('--------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "117362/117362 [==============================] - 525s 4ms/step - loss: 0.6689 - acc: 0.8525\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.66886, saving model to weights.hdf5\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "самые красивые девушки учатся на матфаке и ходят в фирменных футболках имит ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "красивые самые красивые девушки учатся на матфаке и ходят в фирменных футболках имит ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "великолепные самые красивые девушки учатся на матфаке и ходят в фирменных футболках имит ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "у нас ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "Epoch 2/100\n",
      "117362/117362 [==============================] - 487s 4ms/step - loss: 0.1555 - acc: 0.9712\n",
      "\n",
      "Epoch 00002: loss improved from 0.66886 to 0.15552, saving model to weights.hdf5\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "самые и и ходят в фирменных футболках имит ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "красивые ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "--------------------------------------\n",
      "великолепные помните , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "у нас ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "Epoch 3/100\n",
      "117362/117362 [==============================] - 485s 4ms/step - loss: 0.1489 - acc: 0.9720\n",
      "\n",
      "Epoch 00003: loss improved from 0.15552 to 0.14886, saving model to weights.hdf5\n",
      "\n",
      "----- Generating text after Epoch: 3\n",
      "самые красивые девушки учатся на матфаке и ходят в фирменных футболках имит ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "красивые , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "великолепные помните , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "у нас ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "Epoch 4/100\n",
      "117362/117362 [==============================] - 485s 4ms/step - loss: 0.1461 - acc: 0.9727\n",
      "\n",
      "Epoch 00004: loss improved from 0.14886 to 0.14605, saving model to weights.hdf5\n",
      "\n",
      "----- Generating text after Epoch: 4\n",
      "самые красивые девушки учатся на матфаке и ходят в фирменных футболках имит ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "красивые у нас '' постоянно пополняется , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "великолепные ведь помните , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "у нас '' постоянно пополняется , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "Epoch 5/100\n",
      "117362/117362 [==============================] - 475s 4ms/step - loss: 0.1460 - acc: 0.9730\n",
      "\n",
      "Epoch 00005: loss improved from 0.14605 to 0.14602, saving model to weights.hdf5\n",
      "\n",
      "----- Generating text after Epoch: 5\n",
      "самые красивые девушки учатся на матфаке и ходят в фирменных футболках имит ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "красивые у нас ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "великолепные помните , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "у нас '' постоянно пополняется , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "Epoch 6/100\n",
      "117362/117362 [==============================] - 457s 4ms/step - loss: 0.1453 - acc: 0.9731\n",
      "\n",
      "Epoch 00006: loss improved from 0.14602 to 0.14528, saving model to weights.hdf5\n",
      "\n",
      "----- Generating text after Epoch: 6\n",
      "самые девушки девушки у нас ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "красивые у нас ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "великолепные самые красивые девушки учатся на матфаке и ходят в фирменных футболках имит ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "у нас ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "Epoch 7/100\n",
      "117362/117362 [==============================] - 469s 4ms/step - loss: 0.1469 - acc: 0.9729\n",
      "\n",
      "Epoch 00007: loss did not improve from 0.14528\n",
      "\n",
      "----- Generating text after Epoch: 7\n",
      "самые красивые девушки учатся на матфаке и ходят в фирменных футболках имит ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "красивые у нас ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "великолепные помните , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "у нас '' постоянно пополняется , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "Epoch 8/100\n",
      "117362/117362 [==============================] - 488s 4ms/step - loss: 0.1474 - acc: 0.9732\n",
      "\n",
      "Epoch 00008: loss did not improve from 0.14528\n",
      "\n",
      "----- Generating text after Epoch: 8\n",
      "самые красивые и удивительные девушки учатся на матфаке ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "красивые на матфаке и ходят в фирменных футболках имит ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "великолепные помните , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "у нас ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "Epoch 9/100\n",
      "117362/117362 [==============================] - 466s 4ms/step - loss: 0.1494 - acc: 0.9730\n",
      "\n",
      "Epoch 00009: loss did not improve from 0.14528\n",
      "\n",
      "----- Generating text after Epoch: 9\n",
      "самые красивые и удивительные девушки учатся на матфаке ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "красивые у нас '' постоянно пополняется , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "великолепные у нас '' постоянно пополняется , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "у нас '' постоянно пополняется , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117362/117362 [==============================] - 464s 4ms/step - loss: 0.1546 - acc: 0.9727\n",
      "\n",
      "Epoch 00010: loss did not improve from 0.14528\n",
      "\n",
      "----- Generating text after Epoch: 10\n",
      "самые красивые девушки учатся на матфаке и ходят в фирменных футболках имит ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "красивые у нас ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "великолепные помните , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "у нас ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "Epoch 11/100\n",
      "117362/117362 [==============================] - 461s 4ms/step - loss: 0.1506 - acc: 0.9732\n",
      "\n",
      "Epoch 00011: loss did not improve from 0.14528\n",
      "\n",
      "----- Generating text after Epoch: 11\n",
      "самые красивые девушки учатся на матфаке и ходят в фирменных футболках имит ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n",
      "красивые у нас '' постоянно пополняется , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "великолепные помните , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "--------------------------------------\n",
      "у нас ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f320632eb00>"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(442)\n",
    "\n",
    "\n",
    "generating_text = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "\n",
    "filepath = \"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, \n",
    "                             monitor='loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "model.fit(X, Y,\n",
    "          batch_size=128,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "        callbacks=[checkpoint, earlystop, generating_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'футболках имит ! ! ! !'"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('футболках', 5, 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'самые прекрасные и удивительные девушки учатся на'"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('самые прекрасные', 5, 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "самые красивые девушки учатся на матфаке и ходят в фирменных футболках имит ! !\n",
      "самые прекрасные и удивительные девушки учатся на матфаке ! ! !\n",
      "девушки у нас '' постоянно пополняется ,\n",
      "удивительные матфаке ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "у нас ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "учатся у нас '' постоянно пополняется , , , ,\n",
      "у нас на матфаке и ходят в фирменных футболках\n"
     ]
    }
   ],
   "source": [
    "for x in [\n",
    "    'самые',\n",
    "    'самые прекрасные',\n",
    "    'девушки',\n",
    "    'удивительные',\n",
    "    'у нас',\n",
    "    'учатся',\n",
    "    'у нас на матфаке'\n",
    "]:\n",
    "    print(generate_text(x, np.random.randint(5, 15), 54))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом мы получили некоторую \"модель языка\" данных постов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
