{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Короткие посты: тетрадка 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Почему бы не попробовать автомазировать создание коротких постов?\" - подумал я, и решил сделать создание постов наподобие \"самые-самые учатся у нас\" более автоматическим.\n",
    "\n",
    "Первой тетрадкой будет тетрадка для добычи постов из группы и поиском кандидаток для таких постов.\n",
    "\n",
    "// данная и последующие тетрадки <br>\n",
    "// преследуют лишь образовательные цели, <br>\n",
    "// в частности, для демонстрации возможностей <br> \n",
    "// библиотек языка Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добываем посты для проекта!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения постов из группы используем эмулятор браузера Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome()#'path to chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://vk.com/imit_omsu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element_by_class_name('ui_tab_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = driver.find_element_by_id('wall_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.clear()\n",
    "row.send_keys('самые красивые девушки\\n')\n",
    "#row.clear()\n",
    "#row.send_keys('девушка месяца\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скроллим до конца!.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 318 ms, sys: 473 ms, total: 791 ms\n",
      "Wall time: 645 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def scroll_to_the_end(driver=driver):\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        pause = np.abs(np.random.normal()) + 0.2\n",
    "        time.sleep(pause)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "scroll_to_the_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы получить html-разметку страницы будем использовать чудесный суп."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = bs.findAll('div', attrs={'class': 'post_content'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_posts_girls(posts):\n",
    "    y = lambda x: str(x).lower()\n",
    "    return [x for x in posts if u'девушк' in y(x) and u'красивые' in y(x) and u'мальчик' not in y(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 20)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts), len(filter_posts_girls(posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "girls = filter_posts_girls(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"wall_post_text\">А ведь самые красивые девушки учатся у нас!</div>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "girls[0].findAll('div', attrs={'class': 'wall_post_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.06 s, sys: 2.12 ms, total: 2.06 s\n",
      "Wall time: 7.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "row.clear()\n",
    "row.send_keys('девушка месяца\\n')\n",
    "\n",
    "time.sleep(2.3)\n",
    "scroll_to_the_end()\n",
    "\n",
    "bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "posts = bs.findAll('div', attrs={'class': 'post_content'})\n",
    "def filter_posts_girls_month(posts):\n",
    "    return [x for x in posts if u'девушк' in str(x).lower() and u'livejournal' in str(x).lower()]\n",
    "len(filter_posts_girls_month(posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним тексты постов и уберем посты с именами в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import NamesExtractor\n",
    "extractor = NamesExtractor()\n",
    "\n",
    "def get_texts(posts):\n",
    "    t = []\n",
    "    for x in posts:\n",
    "        t.extend(list(map(lambda z: z.text, x.findAll('div', attrs={'class': 'wall_post_text'}))))\n",
    "    t = [x for x in t if len(extractor(x).matches) == 0]\n",
    "    return t\n",
    "\n",
    "\n",
    "def save_texts(texts, name='data.txt'):\n",
    "    with open(name, 'w') as f:\n",
    "        f.write('\\n'.join(texts))\n",
    "\n",
    "\n",
    "def load_texts(name='data.txt'):\n",
    "    with open(name) as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = get_texts(girls)\n",
    "save_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = load_texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Позже мы будем использовать эти тексты для генерации подписей для новых постов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ищем героинь для нашего поста!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://vk.com/search?c%5Bgroup%5D=225037&c%5Bsection%5D=people')\n",
    "sex = driver.find_element_by_xpath(\"//div[contains(@onclick,'radiobtn')]\")\n",
    "sex.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64.8 ms, sys: 3.26 ms, total: 68 ms\n",
      "Wall time: 19.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scroll_to_the_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "def get_refs2persons(page):\n",
    "    return ['vk.com' + x.findAll('a')[0].get_attribute_list('href')[0] for x in page.findAll('div', attrs={'class': 'labeled name'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = get_refs2persons(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока не будем фильтровать список и сохраним кандидаток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_texts(candidates, 'candidates.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительно добудем разные хвалебные эпитеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.liveinternet.ru/users/4468278/post246073969/')\n",
    "\n",
    "bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "compliments = [x for x in bs.findAll('p')]\n",
    "compliments = [x for x in compliments if len(x) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "compliments = [x.text.strip() for x in compliments]\n",
    "compliments = [x for x in compliments if len(x) > 0]\n",
    "compliments = [' '.join(word_tokenize(x)) for x in compliments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def preprocess(sentence):\n",
    "    tokenizer = RegexpTokenizer(r'[а-я]+|[А-Я]+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [morph.parse(w)[0].normal_form for w in tokens if morph.tag(w)[0].POS == 'ADJF']\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for c in compliments:\n",
    "    tmp.extend(preprocess(c))\n",
    "tmp = list(set(tmp) ^ {\n",
    "    'наш', \n",
    "    'свой', \n",
    "    'общий', \n",
    "    'непостоянный', \n",
    "    'беспечный', \n",
    "    'эгоистичный', \n",
    "    'сумасшедший', \n",
    "    'весь', \n",
    "    'каждый',\n",
    "    'индивидуальный',\n",
    "    'огнеопасный',\n",
    "    'упоенный',\n",
    "    'другой',\n",
    "    'шипучий',\n",
    "    'тот'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Печально, но не все эпитеты оказались положительной тональности.\n",
    "\n",
    "Поэтому здесь имеет смысл использовать классификатор тональности!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оффтоп: получим ссылки со страницы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "get_some_url = lambda x: re.findall(r'url\\([^)(]+\\)', str(x))\n",
    "get_httplike = lambda x: [y[1:-1] for y in re.findall(r'[\\\"\\']http.*[\\\"\\']', str(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_some_url(girls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list(map(get_some_url, girls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#girls[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "counts = np.array(list(map(len, urls)))\n",
    "#list(zip(counts == 0, range(len(counts))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_httplike(girls[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#httplike = list(map(get_httplike, girls))\n",
    "#counts = np.array(list(map(len, httplike)))\n",
    "#list(zip(counts == 0, range(len(counts))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#girls[8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
